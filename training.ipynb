{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb7db86f9b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import time\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from utils.bert_model import get_dataset, tokenize, avgPoolFunc\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def clean_data():\n",
    "    with open(\"data/bigger sample.json\", \"r\") as file:\n",
    "        with open(\"data/cleanedData.json\", \"w\") as cleaned:\n",
    "            for line in file:\n",
    "                lineJ = json.loads(line)\n",
    "                if lineJ[\"label\"] in [\"Bullish\", \"Bearish\"]:\n",
    "                    cleaned.write(json.dumps(lineJ)+\"\\n\")\n",
    "\n",
    "def get_max_length(cleanedDataFile, tokenizer):\n",
    "    max_length = 0\n",
    "    with open(cleanedDataFile, \"r\") as file:\n",
    "        for line in file:\n",
    "            lineJ = json.loads(line)\n",
    "            max_length = max(max_length, len(tokenizer.encode(lineJ[\"body\"])))\n",
    "    \n",
    "    return max_length\n",
    "\n",
    "class BertFineTuned(nn.Module):\n",
    "    def __init__(self, out_dim, poolFunc, hiddenStates=768):\n",
    "        \"\"\"\n",
    "        out_dim is the dim of the output, default is 2 for sentiment classification\n",
    "        poolFunc is the function for pooling the output embedings of bert, \n",
    "            suggested to use avgPoolFunc\n",
    "        hiddenStates is the dim of output embeddings of bert, default is 768\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        #self.pooler = Pooler(hiddenStates, poolFunc)\n",
    "        self.pooler = nn.Linear(hiddenStates, hiddenStates)\n",
    "        self.dense = nn.Linear(hiddenStates, out_dim)\n",
    "        self.pool_func = poolFunc\n",
    "        self.normalize = nn.BatchNorm1d(hiddenStates)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)[0]\n",
    "        x = self.pool_func(x)\n",
    "        x = self.pooler(x)\n",
    "        x = self.normalize(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dense(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = get_max_length(\"data/cleanedData.json\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigData = get_dataset(\"data/cleanedData.json\", max_length=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSize = len(bigData)\n",
    "trainSize = round(dataSize * 0.9)\n",
    "testSize = dataSize - trainSize\n",
    "train, test = random_split(bigData, [trainSize, testSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23784, 2643)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSize, testSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train, batch_size = 32, shuffle=True, num_workers=10)\n",
    "testloader = DataLoader(test, batch_size=32, num_workers=10)\n",
    "model = BertFineTuned(2, avgPoolFunc).to(device)\n",
    "\n",
    "\n",
    "sgdOptim = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True)\n",
    "adamOptim = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history = []\n",
    "running_loss = 0.0\n",
    "elapsed_time = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.11595141887665 loss at epoch 1, step 50, average time per step: 2.005021057128906\n",
      "32.611062467098236 loss at epoch 2, step 50, average time per step: 3.3772318601608275\n",
      "32.223897993564606 loss at epoch 3, step 50, average time per step: 3.3960593509674073\n",
      "32.086209774017334 loss at epoch 4, step 50, average time per step: 3.3805804586410524\n",
      "31.989273250102997 loss at epoch 5, step 50, average time per step: 3.3869371700286863\n",
      "31.892602145671844 loss at epoch 6, step 50, average time per step: 3.3837448358535767\n",
      "31.802795737981796 loss at epoch 7, step 50, average time per step: 3.3840641260147093\n",
      "31.73996177315712 loss at epoch 8, step 50, average time per step: 3.3805708360671995\n",
      "31.639796525239944 loss at epoch 9, step 50, average time per step: 3.384413547515869\n",
      "31.622190356254578 loss at epoch 10, step 50, average time per step: 3.387392454147339\n",
      "31.488242626190186 loss at epoch 11, step 50, average time per step: 3.38781334400177\n",
      "31.507656276226044 loss at epoch 12, step 50, average time per step: 3.3854299783706665\n",
      "31.446510642766953 loss at epoch 13, step 50, average time per step: 3.389703140258789\n",
      "31.434114634990692 loss at epoch 14, step 50, average time per step: 3.3905334949493406\n",
      "31.446961611509323 loss at epoch 15, step 50, average time per step: 3.384918360710144\n",
      "31.40143448114395 loss at epoch 16, step 50, average time per step: 3.386410117149353\n",
      "31.257372558116913 loss at epoch 17, step 50, average time per step: 3.3838169384002685\n",
      "31.240277737379074 loss at epoch 18, step 50, average time per step: 3.3848827362060545\n",
      "31.034856498241425 loss at epoch 19, step 50, average time per step: 3.3831370067596436\n",
      "30.945700377225876 loss at epoch 20, step 50, average time per step: 3.38304792881012\n",
      "30.885498076677322 loss at epoch 21, step 50, average time per step: 3.385003995895386\n",
      "30.801793217658997 loss at epoch 22, step 50, average time per step: 3.3835733270645143\n",
      "30.696405977010727 loss at epoch 23, step 50, average time per step: 3.384244737625122\n",
      "30.587980300188065 loss at epoch 24, step 50, average time per step: 3.383716187477112\n",
      "30.738885909318924 loss at epoch 25, step 50, average time per step: 3.387742223739624\n",
      "30.71511608362198 loss at epoch 26, step 50, average time per step: 3.387858943939209\n",
      "31.002042412757874 loss at epoch 27, step 50, average time per step: 3.388535280227661\n",
      "30.70719999074936 loss at epoch 28, step 50, average time per step: 3.3889311790466308\n",
      "30.881866842508316 loss at epoch 29, step 50, average time per step: 3.383310809135437\n",
      "30.553057104349136 loss at epoch 30, step 50, average time per step: 3.3869461536407472\n",
      "30.42410719394684 loss at epoch 31, step 50, average time per step: 3.385448384284973\n",
      "30.448055624961853 loss at epoch 32, step 50, average time per step: 3.3861822748184203\n",
      "30.37072029709816 loss at epoch 33, step 50, average time per step: 3.3863136625289916\n",
      "30.26965755224228 loss at epoch 34, step 50, average time per step: 3.385670738220215\n",
      "30.182673811912537 loss at epoch 35, step 50, average time per step: 3.3880739641189574\n",
      "30.137718558311462 loss at epoch 36, step 50, average time per step: 3.3883854055404665\n",
      "30.216710597276688 loss at epoch 37, step 50, average time per step: 3.387261276245117\n",
      "30.240007430315018 loss at epoch 38, step 50, average time per step: 3.3879320192337037\n",
      "30.3350467979908 loss at epoch 39, step 50, average time per step: 3.387205510139465\n",
      "30.102245688438416 loss at epoch 40, step 50, average time per step: 3.384844822883606\n",
      "30.063490837812424 loss at epoch 41, step 50, average time per step: 3.388469214439392\n",
      "29.917345821857452 loss at epoch 42, step 50, average time per step: 3.387023983001709\n",
      "29.779304176568985 loss at epoch 43, step 50, average time per step: 3.3844359922409057\n",
      "29.755184561014175 loss at epoch 44, step 50, average time per step: 3.388363060951233\n",
      "29.534498661756516 loss at epoch 45, step 50, average time per step: 3.38838529586792\n",
      "29.704261392354965 loss at epoch 46, step 50, average time per step: 3.3908377504348755\n",
      "29.569851577281952 loss at epoch 47, step 50, average time per step: 3.388252644538879\n",
      "29.462609142065048 loss at epoch 48, step 50, average time per step: 3.3898256540298464\n",
      "29.347355991601944 loss at epoch 49, step 50, average time per step: 3.3865764570236205\n",
      "29.2071795463562 loss at epoch 50, step 50, average time per step: 3.384598550796509\n"
     ]
    }
   ],
   "source": [
    "for j in range(50):\n",
    "    for i, data in enumerate(testloader):\n",
    "        t1 = time.time()\n",
    "\n",
    "        inputs = data[\"text\"].to(device)\n",
    "        labels = data[\"label\"].to(device)\n",
    "\n",
    "        adamOptim.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        adamOptim.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        elapsed_time += time.time() - t1\n",
    "\n",
    "        if i%50 == 49:\n",
    "            print(f\"{running_loss} loss at epoch {j+1}, step {i+1}, average time per step: {elapsed_time/50}\")\n",
    "            loss_history.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "            elapsed_time = 0.0\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.59375"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2643/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 461, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.bert(sample[\"text\"].cuda())\n",
    "result[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFineTuned(nn.Module):\n",
    "    def __init__(self, out_dim, poolFunc, hiddenStates=768):\n",
    "        \"\"\"\n",
    "        out_dim is the dim of the output, default is 2 for sentiment classification\n",
    "        poolFunc is the function for pooling the output embedings of bert, \n",
    "            suggested to use avgPoolFunc\n",
    "        hiddenStates is the dim of output embeddings of bert, default is 768\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        #self.pooler = Pooler(hiddenStates, poolFunc)\n",
    "        self.pooler = nn.Linear(hiddenStates, hiddenStates//2)\n",
    "        self.dense = nn.Linear(hiddenStates//2, out_dim)\n",
    "        self.pool_func = poolFunc\n",
    "        self.normalize = nn.BatchNorm1d(hiddenStates//2)\n",
    "\n",
    "    def trainable(self):\n",
    "        return [*self.dense.parameters()]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)[0]\n",
    "        x = self.pool_func(x)\n",
    "        x = self.pooler(x)\n",
    "        x = self.normalize(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.dense(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6*60/83.4*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 0\n",
    "total = 0\n",
    "for data in testloader:\n",
    "    inputs = data[\"text\"].to(device)\n",
    "    labels = data[\"label\"].to(device)\n",
    "\n",
    "    output = model(inputs)\n",
    "    predictions = torch.argmax(output, dim=1)\n",
    "    correct = predictions == labels\n",
    "    right += correct.sum().item()\n",
    "    total += labels.size()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right.item()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(sample[\"text\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result, \"data/tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
